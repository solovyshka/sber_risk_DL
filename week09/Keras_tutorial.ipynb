{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Keras это высокоуровневая обертка над tensorflow. Но в текущих реализациях библиотеке tf они живут очень рядом. \n",
    "Когда мы собираем свои нейронки мы берем уже готовые слои из keras и добавляем что-то свое, если нам требуется.\n",
    "Но keras можно использовать без явного использования TF пытаясь свести задачу к fit-predict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "Самое простое, что мы можем сделать это собирать слои последовательно друг за другом - займеся же этим!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers as L # подгружаем нужные модули. \n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в keras лежит несколько наборов данных. Для примера возьмем fashion_mnist - как mnist, но про предметы одежды :)\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10**4, random_state=42)\n",
    "\n",
    "X_train = X_train/ 255.\n",
    "X_val = X_val/ 255.\n",
    "X_test = X_test/ 255.\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для того, чтобы учить через cross_entropy нам нужно сделать OHE таргетам. И эта функция есть в keras!\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_ohe = to_categorical(y_train)\n",
    "y_test_ohe = to_categorical(y_test)\n",
    "y_val_ohe = to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 1s 14us/sample - loss: 1.8986 - categorical_accuracy: 0.4763 - val_loss: 1.4738 - val_categorical_accuracy: 0.6307\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 0s 8us/sample - loss: 1.2020 - categorical_accuracy: 0.6655 - val_loss: 1.0251 - val_categorical_accuracy: 0.6802\n"
     ]
    }
   ],
   "source": [
    "## Первая простая нейронка\n",
    "\n",
    "tf.random.set_seed(42) # фиксируем random_seed\n",
    "\n",
    "model = Sequential(name = 'first_try')\n",
    "model.add(L.Input(shape = (28,28))) # входной нейрон с данными. Его обычно можно опускать, сразу передавая \n",
    "# в нейрон размерность. Но Dense ячейки не умеют работать с картинками, поэтому оставляем Input\n",
    "model.add(L.Flatten()) # разворачиваем картинку в вектор\n",
    "model.add(L.Dense(100,  kernel_initializer='random_normal',name='First')) # можно именовать и потом брать слои по именам\n",
    "model.add(L.ReLU()) # добавляем активацию\n",
    "model.add(L.Dense(10, kernel_initializer = 'random_normal',name='Output'))\n",
    "model.add(L.Softmax())\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4) # так же нам нужно указать оптимайзер\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',\n",
    "             metrics=[\"categorical_accuracy\"]) # и собрать нашу модель, указав метрики,loss и оптимизатор\n",
    "\n",
    "\n",
    "history1 = model.fit(X_train,y_train_ohe,batch_size=500,epochs=2,validation_data = (X_val,y_val_ohe))\n",
    "# и процесс обучения. Задаем количество эпох, размер батча и валидационную часть наших данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 1s 15us/sample - loss: 1.8141 - categorical_accuracy: 0.4979 - val_loss: 1.3973 - val_categorical_accuracy: 0.6449\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 0s 8us/sample - loss: 1.1440 - categorical_accuracy: 0.6720 - val_loss: 0.9782 - val_categorical_accuracy: 0.6839\n"
     ]
    }
   ],
   "source": [
    "# Эту же модель можно записать чуть в меньшее количество строчек кода\n",
    "model = Sequential(name = 'first_try')\n",
    "model.add(L.Input(shape = (28,28))) \n",
    "model.add(L.Flatten()) \n",
    "model.add(L.Dense(100,  kernel_initializer='random_normal',name='First',activation='relu')) # можно именовать и потом брать слои по именам\n",
    "model.add(L.Dense(10, kernel_initializer = 'random_normal',name='Output',activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4) \n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',\n",
    "             metrics=[\"categorical_accuracy\"]) \n",
    "\n",
    "history1 = model.fit(X_train,y_train_ohe,batch_size=500,epochs=2,validation_data = (X_val,y_val_ohe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?L.Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из приятного - все в таком подходе можно кастомизировать под себя!\n",
    "Но пока продолжим рассмотрение о следующем подходе сборки моделей.\n",
    "Класс Sequential не дает нам вообще никакой гибкости, позволяя набирать слои только последовательно.\n",
    "Что же у нас есть новый герой  - Model.\n",
    "Он позволяет собирать сетки практически любой архитектуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model  # подгружаем нужные модули. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = 'uniform'\n",
    "act = 'relu'\n",
    "\n",
    "input_tensor = L.Input(shape=(28, 28)) # задаем вход\n",
    "x = L.Flatten()(input_tensor)# применение нейрона к входу\n",
    "x = L.Dense(100, kernel_initializer=init, activation=act)(x) # повторяем всю логику сколько нам надо\n",
    "x = L.Dense(100, kernel_initializer=init, activation=act)(x)\n",
    "output_tensor = L.Dense(10, kernel_initializer=init, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(input_tensor, output_tensor) # Keras под копотом сам собирает граф.\n",
    "# Если он может получить из входа выхода то вы великолепны.\n",
    "\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',\n",
    "             metrics=[\"categorical_accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 1s 15us/sample - loss: 1.7141 - categorical_accuracy: 0.4943 - val_loss: 1.1252 - val_categorical_accuracy: 0.6275\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 0s 8us/sample - loss: 0.9337 - categorical_accuracy: 0.6636 - val_loss: 0.8333 - val_categorical_accuracy: 0.6795\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train_ohe,batch_size=500,epochs=2,validation_data = (X_val,y_val_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход позволяет делать практически любой гибкости нейронки. Как пример - двухголовая!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 28, 28)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 28, 28)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 784)          0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 784)          0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 100)          78500       flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 100)          78500       flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 100)          10100       dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 100)          10100       dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           dense_30[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 10)           2010        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 179,210\n",
      "Trainable params: 179,210\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = L.Input(shape=(28, 28))\n",
    "input_2 = L.Input(shape=(28, 28))\n",
    "\n",
    "x1 = L.Flatten()(input_1)\n",
    "x1 = L.Dense(100, kernel_initializer=init, activation=act)(x1)\n",
    "x1 = L.Dense(100, kernel_initializer=init, activation=act)(x1)\n",
    "\n",
    "x2 = L.Flatten()(input_2)\n",
    "x2 = L.Dense(100, kernel_initializer=init, activation=act)(x2)\n",
    "x2 = L.Dense(100, kernel_initializer=init, activation=act)(x2)\n",
    "\n",
    "x = L.concatenate([x1, x2]) # Волшебное слово, которое позволяет нам соеденять несколько потоков наших данных\n",
    "output = L.Dense(10, kernel_initializer=init, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model([input_1, input_2], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,loss='categorical_crossentropy',\n",
    "             metrics=[\"categorical_accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 1s 22us/sample - loss: 1.3714 - categorical_accuracy: 0.6164 - val_loss: 0.8260 - val_categorical_accuracy: 0.6711\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 1s 14us/sample - loss: 0.7213 - categorical_accuracy: 0.7305 - val_loss: 0.6702 - val_categorical_accuracy: 0.7558\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train,X_train],y_train_ohe,batch_size=500,epochs=2,validation_data = ([X_val,X_val],y_val_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нужно для винды, если не видит путь до graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model) # можно нарисовать модельку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Несколько выходов и функций потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 28, 28)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 784)          0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 100)          78500       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 100)          78500       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 100)          78500       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gender (Dense)                  (None, 1)            101         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "income (Dense)                  (None, 10)           1010        dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "age (Dense)                     (None, 1)            101         dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 236,712\n",
      "Trainable params: 236,712\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init = 'uniform'\n",
    "act = 'relu'\n",
    "\n",
    "input_tensor = L.Input(shape=(28, 28))\n",
    "\n",
    "x = L.Flatten()(input_tensor)\n",
    "x1 = L.Dense(100, kernel_initializer=init, activation=act)(x)\n",
    "x2 = L.Dense(100, kernel_initializer=init, activation=act)(x)\n",
    "x3 = L.Dense(100, kernel_initializer=init, activation=act)(x)\n",
    "\n",
    "output_1 = L.Dense(1, kernel_initializer=init, activation='sigmoid',name='gender')(x1)\n",
    "output_2 = L.Dense(10, kernel_initializer=init, activation='softmax',name='income')(x2)\n",
    "output_3 = L.Dense(1, kernel_initializer=init,name='age')(x3)\n",
    "\n",
    "model = keras.Model(input_tensor, [output_1, output_2, output_3])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы модель не переобучилась под самую большую функцию потерь\n",
    "# их можно взвесить\n",
    "model.compile(optimizer='adam', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "                                    loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "\n",
    "# если дали выходам имена, можно вот так: \n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "\n",
    "              loss={'age': 'mse',\n",
    "                    'income': 'categorical_crossentropy',\n",
    "                    'gender': 'binary_crossentropy'},\n",
    "                    \n",
    "              loss_weights={'age': 0.25,\n",
    "                            'income': 1.,\n",
    "                            'gender': 10.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помните статью про то, как люди рисовали функции потерь? [Теперь появилась галерея!](https://losslandscape.com/gallery/) Есть подход как skip-connection, он сильно меняет нашу функцию.\n",
    "\n",
    "![](https://i.stack.imgur.com/UDvbg.png)\n",
    "\n",
    "Такую модель нельзя собрать через `Sequence`-стиль, но можно через функциональный стиль. Давайте попробуем сделать это. Заодно посмотрим насколько сильно в нашей ситуации будет меняться траектория обучения. (Сравним обычный 6-ти слойный персептрон и с прокидыванием инфы, например со 2 слоя на 5ый). Ну и также сразу поэксперементируем с функциями активаций и batchnorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ  создайте сеть по инструкциям ниже\n",
    "tf.random.set_seed(42)\n",
    "## Соберите обычный 6ти слойный перспептрон\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Соберите обычный 6ти слойный перспептрон с функциями активации сигмоид\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Соберите с батчнормом и релу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Соберите ну и наконец прокидывание данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для удобной отрисовки всего\n",
    "\n",
    "def plot_history(histories, key='loss', start=0):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch[start:], history.history['val_'+key][start:],\n",
    "                       #'--', \n",
    "                       label=name.title()+' Val')\n",
    "            #plt.plot(history.epoch[start:], history.history[key][start:], color=val[0].get_color(),\n",
    "            #     label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([start, max(history.epoch)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([('simple',    history_simple), \n",
    "              ('simple_sigmoid', history_simple_sigmoid),\n",
    "              ('simple_BN_and_init', history_simple_BN_and_init),\n",
    "              ('complex!!!',   history_complex)\n",
    "             ],\n",
    "             start=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Более интересная часть - кастомизация\n",
    "В keras уже есть несколько удобных callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.callbacks.ModelCheckpoint"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EarlyStopping # останавливает обучение если наша метрика не меняет n эпох\n",
    "LearningRateScheduler # меняет наш learning_rate по расписанию\n",
    "ReduceLROnPlateau # понижает на LR если не происходит улучшения\n",
    "ModelCheckpoint # сохраняет нашу лучшую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=3)\n",
    "reduce_on_plateau = ReduceLROnPlateau(patience=3)\n",
    "# filepath=\"checkpoint_path/weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"checkpoint_path/weights-improvement.hdf5\"\n",
    "model_checkpoing = ModelCheckpoint(filepath,\n",
    "                                   save_best_only=True,\n",
    "                                  save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    model = Sequential(name = 'simple_model')\n",
    "    model.add(L.Input(shape = (28,28))) \n",
    "    model.add(L.Flatten()) \n",
    "    model.add(L.Dense(100,  kernel_initializer='random_normal',name='First',activation='relu'))\n",
    "    model.add(L.Dense(100,  kernel_initializer='random_normal',name='Second',activation='relu'))\n",
    "    model.add(L.Dense(10, kernel_initializer = 'random_normal',name='Output',activation='softmax'))\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4) \n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',\n",
    "                 metrics=[\"categorical_accuracy\"]) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = create_simple_model()\n",
    "\n",
    "history = model.fit(X_train,y_train_ohe,batch_size=500,epochs=20,\n",
    "                     validation_data = (X_val,y_val_ohe),\n",
    "                    callbacks = [early_stop,reduce_on_plateau,model_checkpoing],verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoint_path/weights-improvement.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=\"checkpoint_path/full_model_improvement.hdf5\"\n",
    "model_checkpoing = ModelCheckpoint(filepath,\n",
    "                                   save_best_only=True,\n",
    "                                  save_weights_only=False)\n",
    "simple_model = create_simple_model()\n",
    "\n",
    "history = simple_model.fit(X_train,y_train_ohe,batch_size=500,epochs=20,\n",
    "                     validation_data = (X_val,y_val_ohe),\n",
    "                    callbacks = [early_stop,reduce_on_plateau,model_checkpoing],verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.4358 - categorical_accuracy: 0.8454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43579934401512144, 0.8454]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.evaluate(x=X_val,y=y_val_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    " \n",
    "class My_Callback(callbacks.Callback):     # Класс My_Callback унаследовал свойства класса Callback\n",
    "    def on_train_begin(self, logs={}):           # Функция, которая выполняется в начале обучения \n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):             # Функция, которая выполняется в конце обучения \n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, logs={}):           # В начале каждой эпохи \n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        # В конце каждой эпохи\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):    # В начале батча\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):      # В конце батча \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "class Printlogs(callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch==10:\n",
    "            print(logs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = create_simple_model()\n",
    "our_callback = Printlogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.47610742747783663, 'categorical_accuracy': 0.83594, 'val_loss': 0.4837979659438133, 'val_categorical_accuracy': 0.8317}\n"
     ]
    }
   ],
   "source": [
    "history = simple_model.fit(X_train,y_train_ohe,batch_size=500,epochs=20,\n",
    "                     validation_data = (X_val,y_val_ohe),\n",
    "                    callbacks = [our_callback],\n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000149011612\n",
      "0.05000000074505806\n",
      "0.02500000037252903\n",
      "0.012500000186264515\n",
      "0.0062500000931322575\n",
      "0.0031250000465661287\n",
      "0.0015625000232830644\n",
      "0.0007812500116415322\n",
      "0.0003906250058207661\n",
      "0.00019531250291038305\n",
      "9.765625145519152e-05\n",
      "4.882812572759576e-05\n",
      "2.441406286379788e-05\n",
      "1.220703143189894e-05\n",
      "6.10351571594947e-06\n",
      "3.051757857974735e-06\n",
      "1.5258789289873675e-06\n",
      "7.629394644936838e-07\n",
      "3.814697322468419e-07\n",
      "1.9073486612342094e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Напишем изменение скорости обучения\n",
    "INIT_LR=0.1\n",
    "# Стратегия для понижения скорости\n",
    "def lr_scheduler(epoch):\n",
    "    drop = 0.5\n",
    "    epochs_drop = 1.0\n",
    "    lrate = INIT_LR * np.math.pow(drop, np.math.floor((epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(lr_scheduler)\n",
    "# класс чтобы отслеживать бесчинства\n",
    "class Print_lr(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(float(tf.keras.backend.get_value(self.model.optimizer.lr)))\n",
    "        # чтобы установить свой LR надо указать\n",
    "        # LR_OUR = ....\n",
    "        # tf.keras.backend.set_value(self.model.optimizer.lr, LR_OUR)\n",
    "\n",
    "\n",
    "simple_model = create_simple_model()\n",
    "history = simple_model.fit(X_train,y_train_ohe,batch_size=500,epochs=20,\n",
    "                     validation_data = (X_val,y_val_ohe),\n",
    "                    callbacks = [lrate,Print_lr()],\n",
    "                           verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте заставим модель посчитать метрики каждую эпоху, как пример\n",
    "# Считать будем на X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомные loss и метрики\n",
    "Что уже есть\n",
    "https://keras.io/api/metrics/\n",
    "\n",
    "https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius    = np.array([-40, -10,  0,  8, 15, 22,  38],  dtype=float)\n",
    "fahrenheit = np.array([-40,  14, 32, 46, 59, 72, 100],  dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 427 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18c95481dc8>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(L.Dense(1))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam( )\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "model.fit(celsius, fahrenheit,  epochs=3, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7 samples\n",
      "Epoch 1/3\n",
      "7/7 [==============================] - 0s 26ms/sample - loss: 1291.6083\n",
      "Epoch 2/3\n",
      "7/7 [==============================] - 0s 143us/sample - loss: 1290.7059\n",
      "Epoch 3/3\n",
      "7/7 [==============================] - 0s 142us/sample - loss: 1289.8048\n",
      "Wall time: 233 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18c92d08fc8>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(L.Dense(1))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam( )\n",
    "\n",
    "model.compile(loss=custom_loss_function, optimizer=opt)\n",
    "model.fit(celsius, fahrenheit,  epochs=3,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## тоже самое можно делать и с метриками. Также можно следить сразу за несколькими метриками, что бывает полезно.\n",
    "## Если хотим добавить совсем сложную логику то мы это будем делать через callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой тетрадке немного поработаем с градусами по цельсию и фаренгейту! Снова попробуем восстановить формулу \n",
    "\n",
    "$$ f = c \\times 1.8 + 32 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18cba97f1c8>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## возьмем срезы\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "model = Sequential()\n",
    "model.add(L.Dense(1,name='our_neural'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(0.1 )\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "model.fit(celsius, fahrenheit,  epochs=600, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sequential_25/our_neural/kernel:0' shape=(1, 1) dtype=float64, numpy=array([[1.80844236]])>,\n",
       " <tf.Variable 'sequential_25/our_neural/bias:0' shape=(1,) dtype=float64, numpy=array([30.71169356])>]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## элементарная задача, но из-за того, что данные не скалированы сходились вечность\n",
    "our_layer = model.get_layer(name='our_neural')\n",
    "our_layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 1), dtype=float64, numpy=\n",
       "array([[-41.62600081],\n",
       "       [ 12.62726997],\n",
       "       [ 30.71169356],\n",
       "       [ 45.17923244],\n",
       "       [ 57.83832895],\n",
       "       [ 70.49742546],\n",
       "       [ 99.43250321]])>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_layer(np.array(celsius).reshape((7,1))) ## Берем прогнозы от слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Есть понимание, как взять определить в callback на какой эпохе мы получили правильное значение весов? Проверить как ведет себя наша нейронка? (с учетом того, что мы точно знаем формулу)\n",
    " Ну и заодно быстро проверить гипотезу - а поможет ли нам batchnorm в данной ситуации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пишем класс нейронки с TF и keras вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# транспонировали выборку\n",
    "x_train = celsius[:,None]\n",
    "y_train = fahrenheit[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "class Super_puper_neural_net(Model):\n",
    "    \n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(Super_puper_neural_net, self).__init__()\n",
    "        self.fc1 = L.Dense(n_hidden_neurons, kernel_initializer='glorot_uniform',\n",
    "                           activation='relu', trainable=True)\n",
    "        self.fc2 = L.Dense(n_hidden_neurons, kernel_initializer='glorot_uniform',\n",
    "                           trainable=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 1), dtype=float64, numpy=\n",
       "array([[ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 3.38618427],\n",
       "       [ 6.34909551],\n",
       "       [ 9.31200675],\n",
       "       [16.0843753 ]])>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_super = Super_puper_neural_net(1)\n",
    "model_super.encode(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ошибка для модели\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean((y_pred-y_true)**2)\n",
    "\n",
    "# оптимизатор \n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# процесс оптимизации\n",
    "def model_train(X, Y):\n",
    "\n",
    "    # находим loss и пробрасываем градиент\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = model_super.encode(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "\n",
    "    # Вычисляем градиенты\n",
    "    gradients = g.gradient(loss, model_super.variables)\n",
    "    \n",
    "    # Обновляем веса a и b в ходе одной итерации спуска \n",
    "    optimizer.apply_gradients(zip(gradients, model_super.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2452.378595\n",
      "step: 100, loss: 1908.733339\n",
      "step: 200, loss: 1512.925791\n",
      "step: 300, loss: 1224.864457\n",
      "step: 400, loss: 1015.159668\n",
      "step: 500, loss: 862.328840\n",
      "step: 600, loss: 750.747670\n",
      "step: 700, loss: 669.096943\n",
      "step: 800, loss: 654.442778\n",
      "step: 900, loss: 630.576029\n"
     ]
    }
   ],
   "source": [
    "#Обучение\n",
    "epochs = 1000 # число эпох \n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Делаем щаг градиентного спуска \n",
    "    model_train(x_train, y_train)\n",
    "    \n",
    "    # Каждую сотую итерацию следим за тем, что произошло\n",
    "    if i%100 == 0:\n",
    "        y_pred = model_super.encode(x_train)\n",
    "        loss_val = mean_square(y_pred, y_train)\n",
    "        print(\"step: %i, loss: %f\" % (i, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Свой слой на Tensorflow для Keras\n",
    "\n",
    "Новые слои можно писать на основе керасовского класса `Layer`. Если прописать `help(tf.keras.layers.Layer)`, можно почитать про него. Если в кратце, нужно реализовать три части: \n",
    "\n",
    "* Конструктор, в нём мы описываем гиперпараметры \n",
    "* Метод `build`, в которм мы описываем все переменные \n",
    "* Метод `call`, который делает forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(L.Layer):\n",
    "    \n",
    "    # Задаём консруктор \n",
    "    def __init__(self, units=32):\n",
    "        super(MyLinear, self).__init__()  # чтобы коректно унаследовались методы\n",
    "        self.units = units                # число нейронов\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # add_weight внутри build то же самое что и Variable, но совместимо с Keras\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal', \n",
    "                                 trainable=True)\n",
    "        \n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal', \n",
    "                                 trainable=True)\n",
    "\n",
    "    # Применение \n",
    "    def call(self, inputs):\n",
    "        # сразу делаем и линейное преобразование и ReLU (а почему бы и нет)\n",
    "        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = Sequential(name = 'simple_model')\n",
    "model_custom.add(L.Input(shape = (28,28))) \n",
    "model_custom.add(L.Flatten()) \n",
    "model_custom.add(L.Dense(100,  kernel_initializer='random_normal',name='First',activation='relu'))\n",
    "model_custom.add(MyLinear()) ### Самый красивый слой\n",
    "model_custom.add(L.Dense(10, kernel_initializer = 'random_normal',name='Output',activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4) \n",
    "model_custom.compile(optimizer=opt,loss='categorical_crossentropy',\n",
    "             metrics=[\"categorical_accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.6633 - categorical_accuracy: 0.7723 - val_loss: 0.6594 - val_categorical_accuracy: 0.7683\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.6257 - categorical_accuracy: 0.7865 - val_loss: 0.6268 - val_categorical_accuracy: 0.7819\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.5967 - categorical_accuracy: 0.7961 - val_loss: 0.6017 - val_categorical_accuracy: 0.7942\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.5729 - categorical_accuracy: 0.8044 - val_loss: 0.5803 - val_categorical_accuracy: 0.8012\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.5536 - categorical_accuracy: 0.8101 - val_loss: 0.5622 - val_categorical_accuracy: 0.8095\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.5367 - categorical_accuracy: 0.8155 - val_loss: 0.5476 - val_categorical_accuracy: 0.8146\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.5224 - categorical_accuracy: 0.8209 - val_loss: 0.5357 - val_categorical_accuracy: 0.8173\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.5102 - categorical_accuracy: 0.8252 - val_loss: 0.5225 - val_categorical_accuracy: 0.8232\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.4996 - categorical_accuracy: 0.8285 - val_loss: 0.5144 - val_categorical_accuracy: 0.8245\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.4899 - categorical_accuracy: 0.8327 - val_loss: 0.5033 - val_categorical_accuracy: 0.8280\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.4817 - categorical_accuracy: 0.8354 - val_loss: 0.4955 - val_categorical_accuracy: 0.8299\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.4740 - categorical_accuracy: 0.8376 - val_loss: 0.4882 - val_categorical_accuracy: 0.8320\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.4669 - categorical_accuracy: 0.8397 - val_loss: 0.4820 - val_categorical_accuracy: 0.8333\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.4618 - categorical_accuracy: 0.8411 - val_loss: 0.4781 - val_categorical_accuracy: 0.8329\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.4558 - categorical_accuracy: 0.8432 - val_loss: 0.4718 - val_categorical_accuracy: 0.8348\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.4505 - categorical_accuracy: 0.8447 - val_loss: 0.4661 - val_categorical_accuracy: 0.8373\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 1s 12us/sample - loss: 0.4458 - categorical_accuracy: 0.8460 - val_loss: 0.4643 - val_categorical_accuracy: 0.8375\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.4414 - categorical_accuracy: 0.8478 - val_loss: 0.4575 - val_categorical_accuracy: 0.8405\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 1s 11us/sample - loss: 0.4371 - categorical_accuracy: 0.8488 - val_loss: 0.4540 - val_categorical_accuracy: 0.8398\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 1s 13us/sample - loss: 0.4339 - categorical_accuracy: 0.8503 - val_loss: 0.4507 - val_categorical_accuracy: 0.8409\n"
     ]
    }
   ],
   "source": [
    "history = model_custom.fit(X_train,y_train_ohe,batch_size=500,epochs=20,\n",
    "                     validation_data = (X_val,y_val_ohe),\n",
    "                    callbacks = [early_stop,reduce_on_plateau],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и нам остался пример, как взять срез модели. Посмотреть прогнозы в середине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train_ohe,batch_size=500,epochs=20,\n",
    "                     validation_data = (X_val,y_val_ohe),\n",
    "                    callbacks = [early_stop,reduce_on_plateau],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем выходы верхних 2х слоев\n",
    "layer_outputs = [layer.output for layer in model.layers[1:3]]\n",
    "# создаем модель, которая вернет эти выходы с учетом заданнаго входа\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = activation_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.38649075, 3.54336526, 3.78493653, ..., 4.28923992, 2.59533527,\n",
       "         2.33044051],\n",
       "        [0.84572429, 4.38319305, 2.8557391 , ..., 2.79402664, 1.15542673,\n",
       "         1.56231554],\n",
       "        [1.88866113, 0.49722533, 0.48871935, ..., 3.56315018, 4.26814976,\n",
       "         1.25973122],\n",
       "        ...,\n",
       "        [1.64454343, 0.61663136, 0.        , ..., 0.94446987, 1.6462323 ,\n",
       "         4.40703848],\n",
       "        [1.66231301, 0.92270261, 1.71392342, ..., 2.53571798, 2.92817679,\n",
       "         1.16723328],\n",
       "        [1.46921835, 0.65876474, 1.72670742, ..., 3.23719186, 3.7936972 ,\n",
       "         1.68555778]]),\n",
       " array([[5.5452259 , 1.21933981, 3.04301914, ..., 0.37099667, 3.9048873 ,\n",
       "         4.75640925],\n",
       "        [6.6589373 , 0.96440545, 4.043728  , ..., 0.        , 6.31727883,\n",
       "         6.48483602],\n",
       "        [5.0319833 , 0.12222322, 0.        , ..., 2.18646237, 0.84729714,\n",
       "         3.49273428],\n",
       "        ...,\n",
       "        [0.        , 5.69175408, 0.25687616, ..., 0.31629389, 1.55848377,\n",
       "         0.        ],\n",
       "        [3.55830517, 1.91132738, 0.        , ..., 0.70235578, 0.03990178,\n",
       "         2.9786485 ],\n",
       "        [4.10624084, 1.21189946, 0.        , ..., 1.34374243, 0.6072686 ,\n",
       "         3.42274117]])]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'First_31/Identity:0' shape=(None, 100) dtype=float64>,\n",
       " <tf.Tensor 'Second_29/Identity:0' shape=(None, 100) dtype=float64>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Что сегодня не вошло - как переопределить градиенты для своих слоев (на уровне keras очень геморойно, если уже занимаетесь этим то вряд ли пишете на верхнеуровневом фраемворке)\n",
    "Как работать с уже готовыми и обучеными моделями, дофичивать нейронки по кусочкам. Но это уже в следующих сериях :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
