{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Слямзино с  Copyright 2019 The TensorFlow Authors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "Будем создавать свой автопереводчик. На основе документации tf попробуем осознать, как работает внимание.\n",
    "Пример ниже про испанский, попробуем получить тоже самое для русского\n",
    "\n",
    "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
    "\n",
    "Note: This example takes approximately 10 minutes to run on a single P100 GPU. (больше :)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:46.319060Z",
     "iopub.status.busy": "2021-04-02T02:05:46.318253Z",
     "iopub.status.idle": "2021-04-02T02:05:53.942328Z",
     "shell.execute_reply": "2021-04-02T02:05:53.941694Z"
    },
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "Возьмем готовый дата сет http://www.manythings.org/anki/. Данные устроены следующим образом:\n",
    "\n",
    "```\n",
    "May I borrow this book?\tМогу ли я одолжить книгу\n",
    "```\n",
    "Плюс какая-то дичь... (для русского языка в частности)\n",
    "\n",
    "Что нам надо еще сделать с текстом\n",
    "1. добавить *start* и *end* токен в каждое предложении.\n",
    "2. Почистим все от ненужного (разные непонятные знаки).\n",
    "3. Создать пару словарей (dictionaries mapping from word → id and id → word).\n",
    "4. Ну и уровнять предложение по длине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:53.949099Z",
     "iopub.status.busy": "2021-04-02T02:05:53.948369Z",
     "iopub.status.idle": "2021-04-02T02:05:54.223340Z",
     "shell.execute_reply": "2021-04-02T02:05:54.223810Z"
    },
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Путь до файла\n",
    "\n",
    "path_to_file = \"rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:54.230405Z",
     "iopub.status.busy": "2021-04-02T02:05:54.229722Z",
     "iopub.status.idle": "2021-04-02T02:05:54.232166Z",
     "shell.execute_reply": "2021-04-02T02:05:54.231684Z"
    },
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "\n",
    "    w.lower().strip()\n",
    "\n",
    "    # Чистим выборку от разных знаков препинания\n",
    "    # Убираем двойной пробел\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # еще чистим\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,^а-яА-Я]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # добавляем токены начали окончания предложения\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:54.237054Z",
     "iopub.status.busy": "2021-04-02T02:05:54.236326Z",
     "iopub.status.idle": "2021-04-02T02:05:54.238623Z",
     "shell.execute_reply": "2021-04-02T02:05:54.239050Z"
    },
    "id": "opI2GzOt479E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> May I borrow this book ? <end>\n",
      "<start> Могу ли я одолжить эту книгу ? <end>\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "rus_sentence = u\"Могу ли я одолжить эту книгу?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(rus_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:54.244176Z",
     "iopub.status.busy": "2021-04-02T02:05:54.243533Z",
     "iopub.status.idle": "2021-04-02T02:05:54.245253Z",
     "shell.execute_reply": "2021-04-02T02:05:54.245661Z"
    },
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Считали\n",
    "# 2. почистили предложения\n",
    "# 3. Вернули в след формате: [ENGLISH, RUSSIAN, Какая-то фигня]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:54.249732Z",
     "iopub.status.busy": "2021-04-02T02:05:54.249072Z",
     "iopub.status.idle": "2021-04-02T02:05:58.938561Z",
     "shell.execute_reply": "2021-04-02T02:05:58.937916Z"
    },
    "id": "cTbSbBz55QtF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Run . <end>\n",
      "<start> Беги ! <end>\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en,rus,non_know = create_dataset(path_to_file, None)\n",
    "print(en[10])\n",
    "print(rus[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> I felt strong . <end>\n",
      "<start> Я почувствовал себя сильным . <end>\n"
     ]
    }
   ],
   "source": [
    "print(en[10100])\n",
    "print(rus[10100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> Несомненно , для каждого мужчины в этом мире где то есть подходящая женщина , которая может стать ему женой , обратное верно и для женщин . Но если учесть , что у человека может быть максимум несколько сотен знакомых , из которых лишь дюжина , а то и меньше , тех , кого он знает близко , а из этой дюжины у него один или от силы два друга , то можно легко увидеть , что с учетом миллионов живущих на Земле людей , ни один подходящий мужчина , возможно , еще не встретил подходящую женщину . <end>\n"
     ]
    }
   ],
   "source": [
    "print(en[-1])\n",
    "print(rus[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421765"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:58.943609Z",
     "iopub.status.busy": "2021-04-02T02:05:58.942960Z",
     "iopub.status.idle": "2021-04-02T02:05:58.945350Z",
     "shell.execute_reply": "2021-04-02T02:05:58.944751Z"
    },
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # токенизируем\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # добавляем padding \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:58.950087Z",
     "iopub.status.busy": "2021-04-02T02:05:58.949434Z",
     "iopub.status.idle": "2021-04-02T02:05:58.951355Z",
     "shell.execute_reply": "2021-04-02T02:05:58.951753Z"
    },
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # считали, вернули пары\n",
    "    targ_lang, inp_lang,non_known = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Обучение на всех данных длительный процесс. Можем взять кусок данных и надеяться что все заработает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:05:58.956299Z",
     "iopub.status.busy": "2021-04-02T02:05:58.955657Z",
     "iopub.status.idle": "2021-04-02T02:06:00.872187Z",
     "shell.execute_reply": "2021-04-02T02:06:00.871006Z"
    },
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Выбор количества примеров это важная часть\n",
    "num_examples = 300000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file,\n",
    "                                                                num_examples)\n",
    "\n",
    "# считаем максимальные длины всех предложений\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:00.877965Z",
     "iopub.status.busy": "2021-04-02T02:06:00.877301Z",
     "iopub.status.idle": "2021-04-02T02:06:00.885478Z",
     "shell.execute_reply": "2021-04-02T02:06:00.885907Z"
    },
    "id": "4QILQkOs3jFG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000 240000 60000 60000\n"
     ]
    }
   ],
   "source": [
    "# Трейн тест сплит\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:00.890450Z",
     "iopub.status.busy": "2021-04-02T02:06:00.889841Z",
     "iopub.status.idle": "2021-04-02T02:06:00.891656Z",
     "shell.execute_reply": "2021-04-02T02:06:00.892047Z"
    },
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "# преврощение в словари\n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f'{t} ----> {lang.index_word[t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  11, 128, ...,   0,   0,   0],\n",
       "       [  1,   4,  34, ...,   0,   0,   0],\n",
       "       [  1,   7, 550, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  1,   6,  24, ...,   0,   0,   0],\n",
       "       [  1,   6,  92, ...,   0,   0,   0],\n",
       "       [  1,   6, 141, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:00.896593Z",
     "iopub.status.busy": "2021-04-02T02:06:00.895919Z",
     "iopub.status.idle": "2021-04-02T02:06:00.899899Z",
     "shell.execute_reply": "2021-04-02T02:06:00.900291Z"
    },
    "id": "VXukARTDd7MT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6 ----> том\n",
      "78 ----> может\n",
      "25 ----> тебе\n",
      "10 ----> это\n",
      "237 ----> купить\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> tom\n",
      "26 ----> can\n",
      "143 ----> buy\n",
      "13 ----> that\n",
      "35 ----> for\n",
      "6 ----> you\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print()\n",
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:02.295930Z",
     "iopub.status.busy": "2021-04-02T02:06:02.295179Z",
     "iopub.status.idle": "2021-04-02T02:06:02.299622Z",
     "shell.execute_reply": "2021-04-02T02:06:02.300061Z"
    },
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:02.306484Z",
     "iopub.status.busy": "2021-04-02T02:06:02.303896Z",
     "iopub.status.idle": "2021-04-02T02:06:02.343281Z",
     "shell.execute_reply": "2021-04-02T02:06:02.343717Z"
    },
    "id": "qc6-NK1GtWQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([512, 17]), TensorShape([512, 14]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Пример реалзиации энкодера и декодера для машинного перевода можно чекнуть тут [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). Мы же посмотрим на attention [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism). На картинке смотрим, что там происходит с вниманием. Объяснение взято из статьи [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "Мы возвращаем все последовательность скрытых состояний из encoder *(batch_size, max_length, hidden_size)* и последнее скрытые состояния *(batch_size, hidden_size)*.\n",
    "\n",
    "Можем посмотреть на уравнения, которые у нас есть:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "Мы возьме вот такой attention [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) для энкодера. и  можем взять описание того, что есть:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "И наш псевдо-код:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax по умолчанию применяется к последней оси нашего тензора, но здесь мы хотим применить его к * 1-й оси *, так как наши данные имеют следующий вид * (batch_size, max_length, hidden_size) *. Max_length - длина нашей последовательности. И мы хотим дать вес каждому элементу последовательности, поэтому и применяем softmax именно так.\n",
    "\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. \n",
    "* `embedding output` = Эмбедим наш выход.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* И этот вектор мы и отдаем дальше\n",
    "\n",
    "Смотрим на реализацию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:02.350331Z",
     "iopub.status.busy": "2021-04-02T02:06:02.349711Z",
     "iopub.status.idle": "2021-04-02T02:06:02.351948Z",
     "shell.execute_reply": "2021-04-02T02:06:02.351453Z"
    },
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:02.356314Z",
     "iopub.status.busy": "2021-04-02T02:06:02.355683Z",
     "iopub.status.idle": "2021-04-02T02:06:04.086751Z",
     "shell.execute_reply": "2021-04-02T02:06:04.086199Z"
    },
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (512, 17, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (512, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# пример всего того, что у нас получилося\n",
    "# sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.093942Z",
     "iopub.status.busy": "2021-04-02T02:06:04.093325Z",
     "iopub.status.idle": "2021-04-02T02:06:04.094939Z",
     "shell.execute_reply": "2021-04-02T02:06:04.095292Z"
    },
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query_hidden_state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.104567Z",
     "iopub.status.busy": "2021-04-02T02:06:04.103963Z",
     "iopub.status.idle": "2021-04-02T02:06:04.468862Z",
     "shell.execute_reply": "2021-04-02T02:06:04.468297Z"
    },
    "id": "k534zTHiDjQU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (512, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (512, 17, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.477387Z",
     "iopub.status.busy": "2021-04-02T02:06:04.476693Z",
     "iopub.status.idle": "2021-04-02T02:06:04.479107Z",
     "shell.execute_reply": "2021-04-02T02:06:04.478602Z"
    },
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.496328Z",
     "iopub.status.busy": "2021-04-02T02:06:04.485540Z",
     "iopub.status.idle": "2021-04-02T02:06:04.522733Z",
     "shell.execute_reply": "2021-04-02T02:06:04.522041Z"
    },
    "id": "P5UY8wko3jFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (512, 12124)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.530148Z",
     "iopub.status.busy": "2021-04-02T02:06:04.528999Z",
     "iopub.status.idle": "2021-04-02T02:06:04.531358Z",
     "shell.execute_reply": "2021-04-02T02:06:04.531807Z"
    },
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.537629Z",
     "iopub.status.busy": "2021-04-02T02:06:04.536506Z",
     "iopub.status.idle": "2021-04-02T02:06:04.539341Z",
     "shell.execute_reply": "2021-04-02T02:06:04.538675Z"
    },
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "1. Энкодим наш вход\n",
    "2. Выход энкодера засовываем в декодер\n",
    "3. Прогноз декодера используем для расчета лосса и возвращаем скрытое состояния для следующего прогноза\n",
    "4. Используем *teacher forcing* решаем какой инпут дать декодеру дальше.\n",
    "5. *Teacher forcing* это техника когда *target word* используем как *next input* декодер.\n",
    "6. Итоговый результат используем как результат и считаем лосс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.548046Z",
     "iopub.status.busy": "2021-04-02T02:06:04.546948Z",
     "iopub.status.idle": "2021-04-02T02:06:04.549768Z",
     "shell.execute_reply": "2021-04-02T02:06:04.549234Z"
    },
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        # Teacher forcing - скармливаем\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:06:04.557511Z",
     "iopub.status.busy": "2021-04-02T02:06:04.556354Z",
     "iopub.status.idle": "2021-04-02T02:08:53.650446Z",
     "shell.execute_reply": "2021-04-02T02:08:53.649859Z"
    },
    "id": "ddefjBMa3jF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.9351\n",
      "Epoch 1 Batch 100 Loss 2.3342\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "        # сохранялка\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* Все похоже на обучение, только мы не можем подсовывать нормальное слово в декодер.\n",
    "* Как только модель предсказала конец предложения останавливаем обучения\n",
    "* Храним выводы attention на каждом шаге\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:08:53.660425Z",
     "iopub.status.busy": "2021-04-02T02:08:53.659728Z",
     "iopub.status.idle": "2021-04-02T02:08:53.662480Z",
     "shell.execute_reply": "2021-04-02T02:08:53.662920Z"
    },
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:08:53.669558Z",
     "iopub.status.busy": "2021-04-02T02:08:53.668870Z",
     "iopub.status.idle": "2021-04-02T02:08:53.670786Z",
     "shell.execute_reply": "2021-04-02T02:08:53.671189Z"
    },
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "# рисовалка внимания\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:08:53.676258Z",
     "iopub.status.busy": "2021-04-02T02:08:53.675621Z",
     "iopub.status.idle": "2021-04-02T02:08:53.677985Z",
     "shell.execute_reply": "2021-04-02T02:08:53.677475Z"
    },
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input:', sentence)\n",
    "    print('Predicted translation:', result)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')),\n",
    "                                  :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:08:53.681733Z",
     "iopub.status.busy": "2021-04-02T02:08:53.681071Z",
     "iopub.status.idle": "2021-04-02T02:08:53.885040Z",
     "shell.execute_reply": "2021-04-02T02:08:53.884482Z"
    },
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-02T02:08:53.889827Z",
     "iopub.status.busy": "2021-04-02T02:08:53.889126Z",
     "iopub.status.idle": "2021-04-02T02:08:54.176938Z",
     "shell.execute_reply": "2021-04-02T02:08:54.176294Z"
    },
    "id": "WrAM0FDomq3E"
   },
   "outputs": [],
   "source": [
    "translate(u'я очень старый человек .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTe5P5ioMJwN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "* Эксперементы :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
